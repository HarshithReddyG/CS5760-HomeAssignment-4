# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17aAuq8pVRtFrM0KYbIHyKPltKiqS9Q8l
"""

!pip install torch matplotlib seaborn

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns

def scaled_dot_product_attention(Q, K, V, return_weights=True):
    """
    Compute scaled dot-product attention
    Q, K, V: tensors of shape (seq_len, d_k)
    return_weights: if True, also return attention weight matrix
    """
    d_k = Q.size(-1)

    # Step 1: Compute raw scores
    scores = torch.matmul(Q, K.transpose(-2, -1))
    print("Raw scores (before scaling):\n", scores)

    # Step 2: Scale by sqrt(d_k) for numerical stability
    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    print("\nScaled scores (after dividing by sqrt(d_k)):\n", scaled_scores)

    # Step 3: Softmax to get attention weights
    attn_weights = F.softmax(scaled_scores, dim=-1)
    print("\nAttention weights (softmax):\n", attn_weights)

    # Step 4: Multiply weights by V to get final output
    output = torch.matmul(attn_weights, V)

    if return_weights:
        return output, attn_weights
    return output

# Random Q, K, V
seq_len = 4
d_k = 8

torch.manual_seed(42)  # reproducibility
Q = torch.randn(seq_len, d_k)
K = torch.randn(seq_len, d_k)
V = torch.randn(seq_len, d_k)

print("Q:\n", Q)
print("\nK:\n", K)
print("\nV:\n", V)

# Compute attention
output, attn_weights = scaled_dot_product_attention(Q, K, V)

print("\nOutput vectors:\n", output)

plt.figure(figsize=(6,5))
sns.heatmap(attn_weights.detach().numpy(), annot=True, cmap='viridis', fmt=".2f")
plt.title("Attention Weight Matrix")
plt.xlabel("Keys")
plt.ylabel("Queries")
plt.show()